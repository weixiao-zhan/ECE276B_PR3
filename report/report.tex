\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{animate}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ECE276B PR1 Report}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Weixiao Zhan}
\IEEEauthorblockA{
    weixiao-zhan[at]ucsd[dot]edu}
}

\maketitle

\section{Introduction}
This project aims to develop a safe trajectory tracking policy 
for a ground differential-drive robot operating 
under a discrete periodic time horizon.
The trajectory tracking problem is formulated as a 
discounted infinite-horizon stochastic optimal control problem, 
which is a subset of general Markov decision processes.

The control policy must track a desired reference trajectory 
while managing a noisy motion model and avoiding obstacles. 
This project explores two primary approaches: 
receding-horizon certainty equivalent control (CEC) and generalized policy iteration (GPI). 
The performance of both methods is evaluated based on 
tracking accuracy, computational complexity, and collision avoidance capabilities.


\section{Problem Formulation}
\subsection{Environment}
The environment in which the robot operates is has following characteristics:
The \(xy\)-plane is bounded within \([-3, 3] \times [-3, 3]\).
There are two circular obstacles:
Obstacle \( C_1 \) is centered at \((-2, -2)\) with a radius of 0.5.
Obstacle \( C_2 \) is centered at \((1, 2)\) with a radius of 0.5.
Denote the free space:
\[ \mathcal{F} = [-3, 3]^2 \setminus (C_1 \cup C_2) \]

The reference trajectory is a sequence of positions and orientations:
\[ r_t \in [-3,3] \times [-3,3] \times [-\pi, \pi)\]
Note that reference trajectory can overlap with obstacles.
And the control policy need to avoid the obstacles.

\subsection{Motion Model}
The motion model for the differential-drive robot is defined as follows. 
The robot's state at time \( t \), denoted by 
\( {x}_t = ({p}_t, \theta_t) \), 
consists of its position \( {p}_t \in [-3,3]^2 \) and orientation \( \theta_t \in [-\pi, \pi) \). 
The control input \( {u}_t = (v_t, \omega_t) \) 
consists of the linear velocity \( v_t \in [0,1] \) and 
angular velocity \( \omega_t \in [-1,1] \). 
The discrete-time kinematic model, 
derived from Euler discretization with a time interval \( \Delta > 0 \), is:

\[
{x}_{t+1} = {f}({x}_t, {u}_t, {w}_t) = {x}_t + {G}({x}_t) {u}_t + {w}_t
\]
where
\[
{G}({x}_t) = \begin{bmatrix}
\Delta \cos(\theta_t) & 0 \\
\Delta \sin(\theta_t) & 0 \\
0 & \Delta 
\end{bmatrix}
\]
and motion noise \( {w}_t \sim \mathcal{N}(0, \mathrm{diag}(\sigma)^2) \).

\subsection{CEC}
CEC is a suboptimal control scheme that applies, at each stage, 
the control that would be optimal
if the noise variables $w_t$ were fixed at their expected values. 
The main advantage of CEC is that it reduces 
a stochastic optimal control problem to a deterministic optimal control problem, 
which can be solved more effectively. 

Receding-horizon CEC, in addition, 
approximates an infinite-horizon problem by repeatedly solving the following discounted finite-horizon
deterministic optimal control problem at each time step

Formally, CEC solves following optimization problem to find its open-loop control policy, 
apply control $u_\tau$, then use realization of $x_{\tau+1}$ to recompute controls.
\[
\min_{{u}_\tau, \ldots, {u}_{\tau+T-1}} 
q({e}_{\tau+T}) + 
\sum_{t=\tau}^{\tau+T-1} \gamma^{t-\tau} 
\left( 
    \begin{gathered}
        {p}_t^\top Q {p}_t  \\ 
        +\\
        q(1 - \cos(\theta_t))^2 \\
        +\\
        {u}_t^\top R {u}_t 
    \end{gathered} 
\right)
\]
in which,
\[
\begin{aligned}
{e}_{t} 
    &= x_{t} - r_{t} \\
x_{t+1} 
    &= f({x}_t, {u}_t, {0}) \\
{x}_t 
    &\in \mathcal{F} \\
{u}_t
    &\in \mathcal{U} \\
\end{aligned},\ 
\begin{aligned}
    Q &\in \mathbb{R}^{[2\times 2]} \\
    q &\in \mathbb{R} \\
    R &\in \mathbb{R}^{[2\times 2]}
\end{aligned}
\]
and $T$ is the look ahead steps.

\subsection{Non-linear Programming (NLP)}
An NLP program written in the standard form can be solved by an NLP solver.
\[
\begin{aligned}
&\min_{{U}} & & c({U}, {E}) \\
&\text{subject to} & & {U}_{\text{lb}} \leq {U} \leq {U}_{\text{ub}} \\
& & & {h}_{\text{lb}} \leq {h}({U}, {E}) \leq {h}_{\text{ub}}
\end{aligned}
\]
where \( {U} = [{u}_\tau^\top, \ldots, {u}_{\tau+T-1}^\top]^\top \) and \( {E} = [{e}_\tau^\top, \ldots, {e}_{\tau+T}^\top]^\top \).

\subsection{GPI}
General policy iteration is a powerful technique to approach 
infinite horizon planing problem.
GPI consists of iterating two steps:
\subsubsection{policy evaluation} finds $V$ function of a given policy.
\subsubsection{policy improvements} greedy improve the policy.

To use GPI in this problem, discretization or function parametrize are necessary
to computers to handle contiguous state space.

\subsection{Q-value iteration}
Q-value iteration is a equivalent variation of general policy iteration.
Its routines is shown in Algorithm \ref{algo:gpi}.
\begin{algorithm}
    \caption{Q-value iteration}
    \label{algo:gpi}
    \begin{algorithmic}[1] % The number [1] enables line numbering
    \While{$\delta Q > \epsilon$}
        \Comment Q-value iteration
        \State for $\forall x \in \mathcal{X}, u \in \mathcal{U}$
        \State $Q(x,u) = l(x,u) + \gamma \mathbb{E}_{x'\sim p_f}\left[\min_{\forall u' \in \mathcal{U}} Q(x', u')\right]$
        % Your loop code here
    \EndWhile
    \State $\pi(x) \leftarrow \arg\min_u Q(x, u), \forall x$
    \Comment extract policy
    \end{algorithmic}
\end{algorithm}


\section{Technical Approach}

\subsection{CEC: NLP Formulation}
In practice, the CEC problem is converted into standard NLP form and solved using Casadi library.

\subsubsection{optimization variable}
\[
\begin{aligned}
U&=\left[ \begin{matrix}u_{\tau }&...&u_{\tau +T-1}\end{matrix}  \right] &\in R^{[2\times T]}\\ 
U_{lb}&=\left[ \begin{matrix}0&\cdots &0\\ -1&\cdots &-1\end{matrix}  \right] \\ 
U_{ub}&=\left[ \begin{matrix}1&\cdots &1\\ 1&\cdots &1\end{matrix}  \right] 
\end{aligned} 
\]

\subsubsection{objective}
\[
\begin{gathered}
C(U, x_\tau)=\sum^{\tau +T-1}_{t=\tau } \gamma^{t-\tau } \cdot \left[l_{e}\left( x_{t}-r_{t} \right) +l_{u}\left( u_{t} \right)  \right] 
\\ 
s.t. \left\{\begin{aligned}
x_{t}&=f(x_{t-1},u_{t})\\ 
l_{e}(e_{t})&=\underbrace{p^{T}_{t}Qp_{t}}_{A} +\underbrace{q\left( 1-\cos \left( \theta_{t}  \right)  \right)^{2} }_{B} \bigg|e_{t}=\left[ \begin{gathered}p_{t}\\ \theta_{t} \end{gathered}  \right] \\ 
l_{u}(u_{t})&=\underbrace{u^{T}_{t}Ru_{t}}_{C} +\underbrace{\left( u_{t}-u_{t-1} \right)^{T} r\left( u_{t}-u_{t-1} \right) }_{D} \
\end{aligned} \right.
\end{gathered}
\]

The above Formulation hardcoded the motion model $f$, reference trajectory $r_t$ 
and initial state $x_\tau$ into the objective function. Thus this 
NLP is formulated on the fly after the realization of $x_\tau$.

Term $A$ and $B$ regulates the positional error between states and reference trajectory.
Term $C$ regulates big movements, Term $D$ is an added smoothing term.

Because of the discounted formulation, NLP solver may
delaying controls into future as the quadric term $C$ is exponentially decayed in time,
leads to small controls at $\tau$ state.
Meanwhile, only $u_\tau$ is actually used and $u_{\tau+1:}$ are discarded,
term $D$ can address this issue.

\subsubsection{constrains}
\[
\begin{gathered}
\begin{aligned}
h(U, x_\tau)&=\left[ \begin{matrix}x_{\tau +1}&\cdots &x_{\tau +T-1}\\ \| p_{t}-c_{1}\|_{2} &\cdots &\| p_{\tau +T-1}-c_{1}\|_{2} \\ \| p_{t}-c_{2}\|_{2} &\cdots &\| p_{\tau +T-1}-c_{2}\|_2 \end{matrix}  \right] \\ 
h_{lb}&=\left[ \begin{matrix}\left[ \begin{gathered}-3\\ -3\\ -\infty \end{gathered}  \right] &\cdots &\left[ \begin{gathered}-3\\ -3\\ -\infty \end{gathered}  \right] \\ c_{1}.r&\cdots &c_{1}.r\\ c_{2}.r&\cdots &c_{2}.r\end{matrix}  \right] \\ 
h_{ub}&=\left[ \begin{matrix}\left[ \begin{gathered} 3\\  3\\  \infty \end{gathered}  \right] &\cdots &\left[ \begin{gathered} 3\\  3\\  \infty \end{gathered}  \right] \\ \infty&\cdots &\infty\\ \infty&\cdots &\infty\end{matrix}  \right] 
\end{aligned} \\
s.t. \ x_{t} = \left[ \begin{gathered}p_{t}\\ \theta_{t} \end{gathered}  \right]  =f(x_{t-1},u_{t}) 
\end{gathered}
\]

The constrains enforces the states $x_t$ to be within the free space $\mathcal{F}$.
Similar to objective function, the motion model is also hardcoded into the constrain function.

The orientation can by any degree in $\mathbb{R}$, 
since the objective function takes $\cos$ value, which is already periodic. 
There is no need to regulate orientation into mod space of $[-\pi, \pi)$, which can be tricky for solver to handle.

\subsubsection{formation considerations}
Converting CEC into the above NLP without explicitly introducing 
optimization variable on state $x_t$ or error states $e_t$ has several benefits.
\begin{itemize}
    \item no need to add motion model constrains, such as 
    \[-\epsilon < x_{t+1} - f(x_t, u_t) <\epsilon\]
    \item no need to implement error state motion model,
    \item avoids boundary and collision check on error states $e_t$
    \item skipped the stage cost, because it would have the same effect as increasing look ahead step by 1.
\end{itemize}

\subsubsection{hyper-parameter tuning}
In practice, following hyper-parameters produced best results:
\[
\begin{array}{ll}T=20&\gamma = 0.9\\ Q=\left[ \begin{matrix}2&1\\ 1&2\end{matrix}  \right] &q=2\\ R=\left[ \begin{matrix}0.05&0\\ 0&0.05\end{matrix}  \right] &r=\left[ \begin{matrix}0.2&0.1\\ 0.1&0.2\end{matrix}  \right] \end{array} 
\]
Different hyper-parameters and their effect are further discussed in Results section.

\subsection{GPI: GPU Batch Q-value iteration}
Q-value iteration involves computationally intensive updates 
that are highly parallelizable, making GPUs ideal computation hardware. 
In this project, an GPU batch Q-value iteration is implemented 
with PyTorch.

\subsubsection{discretization}
the state space $\mathcal{X} = [-3,3]^2\times[-\pi,\pi)$ is
discretized into $(nx, ny, nth)$ equally-spaced points in each dimension.
The control space $\mathcal{U} = [0,1]\times[-1,1]$ is discretized into $(nv,nw)$
equally-spaced controls.
Since the trajectory has period $\mathcal{T}$, 
the time horizon is discretized in $nt=\mathcal{T}$ steps.

\subsubsection{transaction matrix and probability}
The contiguous and stochastic motion model $f$ can be discretized and described by 
transaction-new-state tensor $P_n$ and transaction-probability tensor $P_p$.
Since the motion model of states $x_t$ is used (not the error state), 
the tensors are \textbf{time invariant}.
Suppose the discretization uses $nn$ neighbors around mean to approximation the Gaussian noise.


$P_n$ is a tensor of shape $(nn, nx, ny, nth, nv, nw, 3)$ and integer data type.
It stores the \textbf{indices} of the coordinates of mean next states $x' = f(x,u, 0) \forall x,u$ and their neighbors

$P_p$ is a tensor of shape $(nn, nx, ny, nth, nv, nw)$ and float data type.
It stores the normalized probabilities of the mean next states and its neighbors.

Fore new states that are out of boundary and causing collision, 
their coordinates are clamped and probabilities set to 0 before normalization.

$P_n$ and $P_p$ are effectively computed using PyTorch batch and broad cast operations.
They are stored into files to save computation between runs.

\subsubsection{stage cost}
$$
\begin{aligned}
L(x_t, u_t) &= 
    p^{T}_{t}Qp_{t} 
    + q\left( 1-\cos \left( \theta_{t}  \right)  \right)^{2} 
    + u^{T}_{t}Ru_{t} \\
s.t. \ \left[ \begin{gathered}p_{t}\\ \theta_{t} \end{gathered}  \right] 
    &= x_{t}-r_{t}
\end{aligned}
$$
The stage cost tensor $L$ is shaped as $(nt, nx, ny, nth, nv, nw)$,
as it dependents on the reference trajectory and is \textbf{time variant}.
For those states that are outsize boundary and causing collision, 
their stage cost are set to  $\infty$

$L$ is also pre-computed and save to file.

\subsubsection{Q iteration}
implements the Algorithm \ref{algo:gpi} using batch operations.
Q values are stored in a $(nt, nx, ny, nth, nv, nw)$ shaped tensor.

The expectation $E_{x' \sim p_f}$ is approximated as weighted sum of 
neighbors' V-value at next time step:
$$
\begin{gathered}
\sum_{k=0}^{nn}P_p[k,...]V_{t+1}[k,...]\\
V_{t+1}[k,...]=\min Q[(t+1)\%(nt), ..., :, :]
\end{gathered}
$$


\subsubsection{implementation considerations}
I discretized the standard states and its motion model instead of the error states.

The primary considerations when I first approached GPI is the memory consumption.
The standard motion model need to discrete $6*6*2\pi*1*2$ hyper-volume 
but the error motion model need to discrete $12*12*2\pi*1*2$ hyper-volume which is 4 times greater.

However, the error state allows easy and uniform adaptive discretization: 
just discrete finer near the origin and coarse on the peripheral, 
which allows fewer cells over the larger hyper-volume
I realized this too late to make adjustments in time, which is left for future work.

\subsubsection{hyper-parameter tuning}
In practice, GPI used the same $Q,q,R$ as CEC. the discretization used following sizes
\[
\begin{array}{lll}nt=100&&\\ nx=31&ny=31&nth=9\\ nv=6&nw=9&\end{array} 
\]
This combination consumes ~12GB of GRAM, which is at the limit of my hardware.

Those infinite stage costs are set to a large number of $2^{14}$, 
as infinity some times cause \texttt{nan} in Cuda

The Q-value iteration is looped until the 
$$\|\delta Q\|_2 < 10$$
before extracting policy. In practice, it usually take ~100 iterations.



\section{Results}
Full animated results can be found at \texttt{fig/}.

\subsection{cec}

\subsection{gpi}

\end{document}
